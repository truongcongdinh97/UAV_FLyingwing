# HÆ°á»›ng Dáº«n Training Model cho Flying Wing UAV

## ðŸ“‹ Tá»•ng Quan

HÆ°á»›ng dáº«n nÃ y cung cáº¥p cÃ¡c bÆ°á»›c chi tiáº¿t Ä‘á»ƒ training custom object detection models cho á»©ng dá»¥ng UAV. Models Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a cho edge inference trÃªn Raspberry Pi vá»›i TensorFlow Lite.

## ðŸ“ Cáº¥u TrÃºc ThÆ° Má»¥c Models

```
models/
â”œâ”€â”€ README.md                    # TÃ i liá»‡u models hiá»‡n cÃ³
â”œâ”€â”€ TRAINING_GUIDE.md           # HÆ°á»›ng dáº«n nÃ y
â”œâ”€â”€ coco_labels.txt             # COCO dataset labels
â”œâ”€â”€ mobilenet_ssd_v2.tflite     # Pretrained model
â”œâ”€â”€ training/                   # Scripts vÃ  configs cho training
â”‚   â”œâ”€â”€ configs/               # Training configurations
â”‚   â”œâ”€â”€ scripts/               # Training scripts
â”‚   â””â”€â”€ utils/                 # Utility functions
â”œâ”€â”€ pretrained/                # Pretrained models tá»« cÃ¡c frameworks
â”‚   â”œâ”€â”€ tensorflow/
â”‚   â”œâ”€â”€ pytorch/
â”‚   â””â”€â”€ yolov5/
â”œâ”€â”€ custom/                    # Custom trained models
â”‚   â”œâ”€â”€ uav_people_detector/
â”‚   â”œâ”€â”€ vehicle_detector/
â”‚   â””â”€â”€ search_rescue/
â”œâ”€â”€ datasets/                  # Training datasets
â”‚   â”œâ”€â”€ raw/                  # Raw images vÃ  annotations
â”‚   â”œâ”€â”€ processed/            # Processed data cho training
â”‚   â””â”€â”€ splits/               # Train/val/test splits
â”œâ”€â”€ checkpoints/              # Training checkpoints
â”‚   â”œâ”€â”€ best_weights/
â”‚   â””â”€â”€ last_weights/
â””â”€â”€ exported/                 # Exported models ready for deployment
    â”œâ”€â”€ tflite/              # TensorFlow Lite models
    â”œâ”€â”€ onnx/                # ONNX format
    â””â”€â”€ openvino/            # OpenVINO format
```

## ðŸš€ Quick Start: Training Custom Model

### BÆ°á»›c 1: Chuáº©n Bá»‹ MÃ´i TrÆ°á»ng

```bash
# Táº¡o virtual environment
python -m venv venv
venv\Scripts\activate  # Windows
# source venv/bin/activate  # Linux/Mac

# CÃ i Ä‘áº·t dependencies
pip install tensorflow==2.13.0
pip install opencv-python==4.8.1
pip install pillow==10.0.0
pip install matplotlib==3.7.2
pip install pycocotools==2.0.7
pip install tqdm==4.66.1
```

### BÆ°á»›c 2: Chuáº©n Bá»‹ Dataset

1. **Collect Images**: Thu tháº­p áº£nh tá»« UAV flights
2. **Annotation**: Label objects vá»›i bounding boxes
3. **Format**: Chuyá»ƒn Ä‘á»•i sang COCO format hoáº·c Pascal VOC

**Dataset Structure**:
```
datasets/uav_custom/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ val/
â”‚   â””â”€â”€ test/
â””â”€â”€ annotations/
    â”œâ”€â”€ train.json  # COCO format
    â”œâ”€â”€ val.json
    â””â”€â”€ test.json
```

### BÆ°á»›c 3: Training vá»›i TensorFlow

```python
# training/train_tensorflow.py
import tensorflow as tf
from tensorflow import keras

# Load pretrained model
base_model = keras.applications.EfficientNetB0(
    include_top=False,
    weights='imagenet',
    input_shape=(320, 320, 3)
)

# Add detection head
# ... (xem script Ä‘áº§y Ä‘á»§ trong training/scripts/)
```

### BÆ°á»›c 4: Convert sang TensorFlow Lite

```python
# training/export_tflite.py
import tensorflow as tf

# Load trained model
model = tf.keras.models.load_model('checkpoints/best_model.h5')

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_keras_model(model)
converter.optimizations = [tf.lite.Optimize.DEFAULT]
converter.target_spec.supported_types = [tf.float16]  # FP16 quantization

tflite_model = converter.convert()

# Save
with open('exported/tflite/custom_model.tflite', 'wb') as f:
    f.write(tflite_model)
```

## ðŸ“Š Dataset Preparation

### 1. Data Collection tá»« UAV

**Recommended Tools**:
- **LabelImg**: GUI tool cho annotation
- **CVAT**: Web-based annotation tool
- **Roboflow**: Cloud-based platform

**Classes cho UAV Applications**:
```yaml
classes:
  - person
  - car
  - truck
  - boat
  - airplane
  - bicycle
  - motorcycle
  - bird
  - dog
  - cat
  - sheep
  - cow
  - horse
```

### 2. Data Augmentation

```python
# training/utils/augmentation.py
import albumentations as A

transform = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
    A.Rotate(limit=15, p=0.3),
    A.RandomScale(scale_limit=0.2, p=0.3),
    A.HueSaturationValue(p=0.3),
], bbox_params=A.BboxParams(format='coco'))
```

### 3. Dataset Splits

```python
# training/utils/split_dataset.py
# Split: 70% train, 15% validation, 15% test
# Äáº£m báº£o class distribution balanced
```

## ðŸ‹ï¸â€â™‚ï¸ Training Strategies

### 1. Transfer Learning

```python
# Sá»­ dá»¥ng pretrained backbone
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(320, 320, 3),
    include_top=False,
    weights='imagenet'
)

# Freeze early layers
for layer in base_model.layers[:100]:
    layer.trainable = False
```

### 2. Multi-Stage Training

**Stage 1**: Fine-tune detection head
**Stage 2**: Fine-tune middle layers
**Stage 3**: Fine-tune entire model vá»›i learning rate tháº¥p

### 3. Hyperparameter Tuning

```yaml
# training/configs/hyperparams.yaml
hyperparameters:
  batch_size: 16
  learning_rate: 0.001
  epochs: 100
  optimizer: "adam"
  loss: "focal_loss"
  
  # Learning rate schedule
  lr_schedule:
    warmup_epochs: 5
    cosine_decay: true
    
  # Data augmentation
  augmentation:
    horizontal_flip: 0.5
    rotation: 15
    brightness: 0.2
    contrast: 0.2
```

## ðŸ”§ Model Architectures

### 1. MobileNet SSD (Recommended cho Raspberry Pi)

**Æ¯u Ä‘iá»ƒm**:
- Nháº¹, fast inference
- PhÃ¹ há»£p cho real-time detection
- Tá»‘t cho edge devices

**Training Command**:
```bash
python training/scripts/train_mobilenet_ssd.py \
  --dataset datasets/uav_custom \
  --epochs 100 \
  --batch_size 16 \
  --output checkpoints/mobilenet_ssd
```

### 2. EfficientDet Lite

**Æ¯u Ä‘iá»ƒm**:
- Balance accuracy vÃ  speed
- State-of-the-art cho mobile devices
- Scalable (Lite0 Ä‘áº¿n Lite4)

### 3. YOLOv5 Nano

**Æ¯u Ä‘iá»ƒm**:
- Ráº¥t nhanh
- Good accuracy cho small objects
- Dá»… training

## ðŸ“ˆ Evaluation Metrics

### 1. Standard Metrics
```python
# training/utils/evaluation.py
metrics = {
    "mAP@0.5": "Mean Average Precision at IoU 0.5",
    "mAP@0.5:0.95": "mAP across IoU thresholds",
    "precision": "TP / (TP + FP)",
    "recall": "TP / (TP + FN)",
    "F1_score": "2 * (precision * recall) / (precision + recall)"
}
```

### 2. UAV-Specific Metrics
```python
metrics_uav = {
    "altitude_robustness": "Performance across altitudes",
    "small_object_detection": "Detection of small objects tá»« high altitude",
    "real_time_fps": "Frames per second trÃªn Raspberry Pi",
    "memory_usage": "RAM consumption"
}
```

## ðŸš€ Deployment Pipeline

### 1. Model Optimization

```python
# training/optimize_model.py
def optimize_for_edge(model_path):
    # Quantization
    converter = tf.lite.TFLiteConverter.from_saved_model(model_path)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    
    # Representative dataset for quantization
    def representative_dataset():
        for _ in range(100):
            data = np.random.rand(1, 320, 320, 3).astype(np.float32)
            yield [data]
    
    converter.representative_dataset = representative_dataset
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.uint8
    converter.inference_output_type = tf.uint8
    
    return converter.convert()
```

### 2. Edge TPU Compilation (Náº¿u cÃ³ Coral TPU)

```bash
# Compile cho Edge TPU
edgetpu_compiler custom_model.tflite --out_dir exported/edgetpu
```

### 3. Integration vá»›i UAV System

```python
# companion_computer/src/ai/object_detector.py
class CustomObjectDetector(ObjectDetector):
    def __init__(self, model_path="models/custom/uav_model.tflite"):
        # Load optimized model
        self.interpreter = tf.lite.Interpreter(model_path=model_path)
        self.interpreter.allocate_tensors()
        
    def detect(self, frame):
        # Preprocess
        input_tensor = self.preprocess(frame)
        
        # Inference
        self.interpreter.set_tensor(self.input_details[0]['index'], input_tensor)
        self.interpreter.invoke()
        
        # Post-process
        boxes = self.interpreter.get_tensor(self.output_details[0]['index'])
        classes = self.interpreter.get_tensor(self.output_details[1]['index'])
        scores = self.interpreter.get_tensor(self.output_details[2]['index'])
        
        return self.postprocess(boxes, classes, scores)
```

## ðŸ“‹ Training Checklist

### TrÆ°á»›c Training
- [ ] Dataset Ä‘Ã£ Ä‘Æ°á»£c labeled vÃ  reviewed
- [ ] Class distribution balanced
- [ ] Train/val/test splits created
- [ ] Data augmentation pipeline ready
- [ ] Evaluation metrics defined

### Trong Training
- [ ] Monitor training loss vÃ  validation metrics
- [ ] Check for overfitting/underfitting
- [ ] Save best checkpoints
- [ ] TensorBoard logging enabled

### Sau Training
- [ ] Evaluate trÃªn test set
- [ ] Optimize model cho edge deployment
- [ ] Test inference speed trÃªn target hardware
- [ ] Document model performance

## ðŸ› ï¸ Utility Scripts

### 1. Dataset Preparation
```bash
python training/scripts/prepare_dataset.py \
  --input datasets/raw \
  --output datasets/processed \
  --format coco
```

### 2. Training
```bash
python training/scripts/train.py \
  --config training/configs/mobilenet_ssd.yaml \
  --gpu 0
```

### 3. Evaluation
```bash
python training/scripts/evaluate.py \
  --model checkpoints/best_model.h5 \
  --dataset datasets/test
```

### 4. Export
```bash
python training/scripts/export_model.py \
  --checkpoint checkpoints/best_model.h5 \
  --output exported/tflite \
  --quantize int8
```

## ðŸ” Debugging Tips

### 1. Common Issues
- **Low Accuracy**: Thá»­ data augmentation, increase dataset size
- **Overfitting**: ThÃªm regularization, dropout, hoáº·c reduce model complexity
- **Slow Inference**: Reduce input size, quantization, hoáº·c sá»­ dá»¥ng lighter model
- **Memory Issues**: Reduce batch size, sá»­ dá»¥ng gradient accumulation

### 2. Performance Optimization
```python
# Enable XLA compilation
tf.config.optimizer.set_jit(True)

# Mixed precision training
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# GPU memory growth
gpus = tf.config.experimental.list_physical_devices('GPU')
for gpu in gpus:
    tf.config.experimental.set_memory_growth(gpu, True)
```

## ðŸ“š Resources

### 1. Pretrained Models
- [TensorFlow Model Zoo](https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2_detection_zoo.md)
- [PyTorch Vision Models](https://pytorch.org/vision/stable/models.html)
- [Ultralytics YOLOv5](https://github.com/ultralytics/yolov5)

### 2. Annotation Tools
- [LabelImg](https://github.com/tzutalin/labelImg)
- [CVAT](https://github.com/openvinotoolkit/cvat)
- [Roboflow](https://roboflow.com)

### 3. Training Platforms
- [Google Colab](https://colab.research.google.com/)
- [Kaggle Notebooks](https://www.kaggle.com/notebooks)
- [AWS SageMaker](https://aws.amazon.com/sagemaker/)

## ðŸŽ¯ Best Practices cho UAV Models

### 1. Altitude-Aware Training
- Train vá»›i images tá»« multiple altitudes
- Augment vá»›i scale variations
- Test across altitude ranges

### 2. Real-Time Constraints
- Target FPS > 15 cho real-time tracking
- Model size < 50MB cho Raspberry Pi
- CPU usage < 80% Ä‘á»ƒ trÃ¡nh overheating

### 3. Field Validation
- Test trong real flight conditions
- Validate vá»›i different lighting conditions
- Test vá»›i motion blur tá»« UAV movement

## ðŸ“ž Support

1. **Issues**: Táº¡o issue trÃªn GitHub repository
2. **Questions**: Check documentation vÃ  examples
3. **Contributions**: Pull requests welcome!

---

**LÆ°u Ã½**: LuÃ´n test model thoroughly trÆ°á»›c khi deploy lÃªn UAV. Safety first!

*Last Updated: December 2025*
*Version: 1.0.0*
